{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e6cefb0049d48a2f4648d752841bb06",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Daniel Acuna <deacuna@syr.edu>\n",
    "- TAs: Tong Zeng <tozeng@syr.edu>, Priya Matnani <psmatnan@syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers either from your classmates or from the internet__\n",
    "- You can put the homework files anywhere you want in your http://notebook.acuna.io workspace but _do not change_ the file names. The TAs and the professor use these names to grade your homework.\n",
    "- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n",
    "- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and TAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before downloading and submitting your work through Blackboard, remember to save and press `Validate` (or go to \n",
    "`Kernel`$\\rightarrow$`Restart and Run All`). \n",
    "- Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages needed for this part\n",
    "# create spark and sparkcontext objects\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import feature, regression, Pipeline, classification, pipeline, evaluation\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark import sql\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image that you are an avid fan of the Chicago Cubs. Somehow, you managed to be part of two email lists, one about baseball (which you are interested in) and hockey (which you are not that interested in). You will use the power of data science to create a classifier that takes an email as an input and predicts whether the email is about baseball or hockey\n",
    "\n",
    "The dataset will be in `email_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fa246e0ba90446b952588ca31d716723",
     "grade": false,
     "grade_id": "cell-6d4e17df71a2afa8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "email_df = spark.read.json('emails.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseball email\n",
      "============================================\n",
      "From: dougb@comm.mot.com (Doug Bank)\n",
      "Subject: Re: Info needed for Cleveland tickets\n",
      "Reply-To: dougb@ecs.comm.mot.com\n",
      "Organization: Motorola Land Mobile Products Sector\n",
      "Distribution: usa\n",
      "Nntp-Posting-Host: 145.1.146.35\n",
      "Lines: 17\n",
      "\n",
      "In article <1993Apr1.234031.4950@leland.Stanford.EDU>, bohnert@leland.Stanford.EDU (matthew bohnert) writes:\n",
      "\n",
      "|> I'm going to be in Cleveland Thursday, April 15 to Sunday, April 18.\n",
      "|> Does anybody know if the Tribe will be in town on those dates, and\n",
      "|> if so, who're they playing and if tickets are available?\n",
      "\n",
      "The tribe will be in town from April 16 to the 19th.\n",
      "There are ALWAYS tickets available! (Though they are playing Toronto,\n",
      "and many Toronto fans make the trip to Cleveland as it is easier to\n",
      "get tickets in Cleveland than in Toronto.  Either way, I seriously\n",
      "doubt they will sell out until the end of the season.)\n",
      "\n",
      "-- \n",
      "Doug Bank                       Private Systems Division\n",
      "dougb@ecs.comm.mot.com          Motorola Communications Sector\n",
      "dougb@nwu.edu                   Schaumburg, Illinois\n",
      "dougb@casbah.acns.nwu.edu       708-576-8207                    \n",
      "\n",
      "Hockey email\n",
      "============================================\n",
      "From: gld@cunixb.cc.columbia.edu (Gary L Dare)\n",
      "Subject: Re: Flames Truly Brutal in Loss\n",
      "Nntp-Posting-Host: cunixb.cc.columbia.edu\n",
      "Reply-To: gld@cunixb.cc.columbia.edu (Gary L Dare)\n",
      "Organization: PhDs In The Hall\n",
      "Distribution: na\n",
      "Lines: 13\n",
      "\n",
      "\n",
      "This game would have been great as part of a double-header on ABC or\n",
      "ESPN; the league would have been able to push back-to-back wins by\n",
      "Le Magnifique and The Great One.  Unfortunately, the only network\n",
      "that would have done that was SCA, seen in few areas and hard to\n",
      "justify as a pay channel. )-;\n",
      "\n",
      "gld\n",
      "--\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~ Je me souviens ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Gary L. Dare\n",
      "> gld@columbia.EDU \t\t\tGO  Winnipeg Jets  GO!!!\n",
      "> gld@cunixc.BITNET\t\t\tSelanne + Domi ==> Stanley\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore the data a bit\n",
    "print(\"Baseball email\\n============================================\")\n",
    "print(email_df.where('target == \"rec.sport.baseball\"').first().email)\n",
    "print(\"Hockey email\\n============================================\")\n",
    "print(email_df.where('target == \"rec.sport.hockey\"').first().email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.1\n",
    "\n",
    "Add a `topic` column to `email_df` to be 1 if `target` is `rec.sport.baseball` and 0 if it is `rec.sport.hockey` and store the result in `email2_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ede01a8c8ef80d25d4fa8d9132b6f9c0",
     "grade": false,
     "grade_id": "cell-79e2472845514523",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "email2_df = email_df.withColumn('topic', (fn.col('target') == \"rec.sport.baseball\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+-----+\n",
      "|               email|email_id|            target|topic|\n",
      "+--------------------+--------+------------------+-----+\n",
      "|From: dougb@comm....|       1|rec.sport.baseball|    1|\n",
      "|From: gld@cunixb....|       2|  rec.sport.hockey|    0|\n",
      "|From: rudy@netcom...|       3|rec.sport.baseball|    1|\n",
      "|From: monack@heli...|       4|  rec.sport.hockey|    0|\n",
      "|Subject: Let it b...|       5|rec.sport.baseball|    1|\n",
      "|From: mmb@lamar.C...|       6|  rec.sport.hockey|    0|\n",
      "|From: fierkelab@b...|       7|rec.sport.baseball|    1|\n",
      "|From: rvpst2+@pit...|       8|  rec.sport.hockey|    0|\n",
      "|From: smorris@ven...|       9|  rec.sport.hockey|    0|\n",
      "|From: richard@amc...|      10|  rec.sport.hockey|    0|\n",
      "|From: brifre1@ac....|      11|  rec.sport.hockey|    0|\n",
      "|From: dwk@cci632....|      12|  rec.sport.hockey|    0|\n",
      "|From: cub@csi.jpl...|      13|rec.sport.baseball|    1|\n",
      "|From: golchowy@al...|      14|  rec.sport.hockey|    0|\n",
      "|From: krattige@hp...|      15|rec.sport.baseball|    1|\n",
      "|From: CROSEN1@ua1...|      16|rec.sport.baseball|    1|\n",
      "|Subject: Re: quic...|      17|rec.sport.baseball|    1|\n",
      "|From: luriem@alle...|      18|rec.sport.baseball|    1|\n",
      "|From: fls@keynes....|      19|rec.sport.baseball|    1|\n",
      "|Subject: Re: Jewi...|      20|rec.sport.baseball|    1|\n",
      "+--------------------+--------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check your results\n",
    "email2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email2_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5ca9ea08f74b120f93fbc878ef4f0532",
     "grade": true,
     "grade_id": "cell-c6048eb3c38d3030",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_array_equal(\n",
    "    email2_df.groupBy('topic').count().orderBy('topic').rdd.map(lambda x: x['count']).collect(),\n",
    "    [600, 597]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.2: tfidf feature engineering\n",
    "Create a pipeline that combines a `Tokenizer`, `CounterVectorizer`, and a `IDF` estimator to compute the tfidf vectors of each email. Fit this pipeline and assign the pipeline transformer to a variable `tfidf_pipeline`. The `Tokenizer` step should create a column `words`, the `CounterVectorizer` step should create a column `tf`, and the `IDF` step should create a column `tfidf`. **Use the default parameers of all the estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "521084e2063144fa97a14bef1e965fca",
     "grade": false,
     "grade_id": "cell-ab24110e63d19470",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create a Pipeline transformer and name it tfidf_pipeline\n",
    "\n",
    "tokenizer = feature.Tokenizer().setInputCol('email').setOutputCol('words')\n",
    "count_vectorizer_estimator = feature.CountVectorizer().setInputCol('words').setOutputCol('tf')\n",
    "IDF_estimator = feature.IDF().setInputCol('tf').setOutputCol('tfidf')\n",
    "tfidf_pipeline= Pipeline(stages=[tokenizer, count_vectorizer_estimator, IDF_estimator]).fit(email2_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "71998580f176786f2247bd666c373fbd",
     "grade": true,
     "grade_id": "cell-cd280a1ccf0f1705",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_array_equal([type(s) for s in tfidf_pipeline.stages],\n",
    "                              [feature.Tokenizer, feature.CountVectorizerModel, feature.IDFModel])\n",
    "np.testing.assert_array_equal(len(tfidf_pipeline.transform(email2_df).collect()), 1197)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Investigate the fitted pieline above and create a variable `lowest_idf` that contain the set of words with the 5 lowest IDF. **Hint: you must extract the vocabulary from the fitted `CountVectorizer` and the IDF values from the fitted `IDF`, both in the stages of `tfidf_pipeline`. You can put both lists into a Pandas dataframe columns and sort by idf, picking 5 after sorting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "21857b2e5609001af62a485bad4ea454",
     "grade": false,
     "grade_id": "cell-18bb787517bd43b1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create lower_idf with list of words with 5 lowest IDF values.\n",
    "#tfidf_pipeline.stages[1].vocabulary,\n",
    "\n",
    "d = {'Vocabulary': tfidf_pipeline.stages[1].vocabulary, 'IDF': tfidf_pipeline.stages[2].idf}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df.nsmallest(5, 'IDF')\n",
    "lowest_idf = set(df[\"Vocabulary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "faade02098f10061ba0107815cc4d81c",
     "grade": true,
     "grade_id": "cell-5493b6e9b55bd45f",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "# it is a set\n",
    "np.testing.assert_equal(type(lowest_idf), set)\n",
    "# it has 5 elements\n",
    "np.testing.assert_equal(len(lowest_idf), 5)\n",
    "# each element is a string\n",
    "np.testing.assert_equal({type(w) for w in lowest_idf}, {str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.3: Compare models\n",
    "\n",
    "Using the following splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89386fe0e5551b31b173749ef8f9d38f",
     "grade": false,
     "grade_id": "cell-2f05175cd6ae7f5c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_df = email2_df.where('email_id < 1197*0.6')\n",
    "validation_df = email2_df.where('email_id >= 1197*0.6 and email_id < 1197*0.9')\n",
    "testing_df = email2_df.where('email_id >= 1197*0.9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[718, 359, 120]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[training_df.count(), validation_df.count(), testing_df.count()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Create pipelines where the first stage is the `tfidf_pipeline` created above and the second stage is a `LogisticRegression` model to predict `target` using different regularization parameters ($\\lambda$) and elastic net mixture ($\\alpha$). *Only change the regularization parameters and leave all parameters as default for `LogisticRegression`*. Fit those pipelines to the appropriate data split.\n",
    "\n",
    "1. Logistic regression with $\\lambda=0$ and $\\alpha=0$ (assign the fitted pipeline to `lr_pipeline1`)\n",
    "2. Logistic regression with $\\lambda=0.02$ and $\\alpha=0.2$ (assign the fitted pipeline to `lr_pipeline2`)\n",
    "3. Logistic regression with $\\lambda=0.1$ and $\\alpha=0.4$ (assign the fitted pipeline to `lr_pipeline3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "02b7fefd0dd482900ba644d56b02400a",
     "grade": false,
     "grade_id": "cell-8db1a50673eea1a7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create lr_pipeline1, lr_pipeline2, and lr_pipeline3\n",
    "lr1 = classification.LogisticRegression(regParam=0, elasticNetParam=0, labelCol = 'topic', featuresCol = 'tfidf')\n",
    "lr_pipeline1 = Pipeline(stages=[tfidf_pipeline, lr1]).fit(training_df)\n",
    "lr2 = classification.LogisticRegression(regParam=0.02, elasticNetParam=0.2, labelCol = 'topic', featuresCol = 'tfidf')\n",
    "lr_pipeline2 = Pipeline(stages=[tfidf_pipeline, lr2]).fit(training_df)\n",
    "lr3 = classification.LogisticRegression(regParam=0.1, elasticNetParam=0.4, labelCol = 'topic', featuresCol = 'tfidf')\n",
    "lr_pipeline3 = Pipeline(stages=[tfidf_pipeline, lr3]).fit(training_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd388632281a19de36557ec7a00af21d",
     "grade": true,
     "grade_id": "cell-bc59c22c523a9016",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (10 pts)\n",
    "np.testing.assert_equal(type(lr_pipeline1), pipeline.PipelineModel)\n",
    "np.testing.assert_equal(type(lr_pipeline2), pipeline.PipelineModel)\n",
    "np.testing.assert_equal(type(lr_pipeline3), pipeline.PipelineModel)\n",
    "np.testing.assert_array_equal([type(s) for s in lr_pipeline1.stages],\n",
    "                              [pipeline.PipelineModel, classification.LogisticRegressionModel])\n",
    "np.testing.assert_array_equal([type(s) for s in lr_pipeline2.stages],\n",
    "                              [pipeline.PipelineModel, classification.LogisticRegressionModel])\n",
    "np.testing.assert_array_equal([type(s) for s in lr_pipeline3.stages],\n",
    "                              [pipeline.PipelineModel, classification.LogisticRegressionModel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Use the evaluator object defined below to compute the area under the curve of your predictors. For example, to compute the area under the curve of pipeline 1 for a dataframe `df`, you would run\n",
    "\n",
    "```python\n",
    "evaluator.evaluate(lr_pipeline1.transform(df))\n",
    "```\n",
    "\n",
    "**You must choose the right data split (dataframe `df`) with the goal of comparing models.**\n",
    "\n",
    "Assign the AUC of the three models to the variables `AUC1`, `AUC2`, and `AUC3`, and and assign the pipeline with the best model to a variable `best_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "59b6e015463ca0fe73c5c25072083486",
     "grade": false,
     "grade_id": "cell-44d4a941d4aef83e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "evaluator = evaluation.BinaryClassificationEvaluator(labelCol='topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the AUC on training of the first model should perfect:\n",
    "\n",
    "```\n",
    "evaluator.evaluate(lr_pipeline1.transform(training_df))\n",
    "```\n",
    "\n",
    "```console\n",
    "1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6d2b1fd1d3f5e9dbcd1986f7c6486293",
     "grade": false,
     "grade_id": "cell-a9d883059572796b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 AUC:  0.9937903626428214\n",
      "Model 2 AUC:  0.988170640834575\n",
      "Model 3 AUC:  0.972180824639841\n"
     ]
    }
   ],
   "source": [
    "# print the AUC for the three models as follows\n",
    "AUC1 = evaluator.evaluate(lr_pipeline1.transform(validation_df))\n",
    "print(\"Model 1 AUC: \", AUC1)\n",
    "AUC2 = evaluator.evaluate(lr_pipeline2.transform(validation_df))\n",
    "print(\"Model 2 AUC: \", AUC2)\n",
    "AUC3 = evaluator.evaluate(lr_pipeline3.transform(validation_df))\n",
    "print(\"Model 3 AUC: \", AUC3)\n",
    "# model to a variable best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1c76a13060b46900aa055920ae5f6a2f",
     "grade": true,
     "grade_id": "cell-29d52d2cec5c8a1e",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_array_equal([type(AUC1), type(AUC2), type(AUC3)],\n",
    "                             [float, float, float])\n",
    "# AUC less than 1\n",
    "np.testing.assert_array_less([AUC1, AUC2, AUC3], [1, 1, 1])\n",
    "# AUC more than 0.5\n",
    "np.testing.assert_array_less([.5, .5, .5],\n",
    "                            [AUC1, AUC2, AUC3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.4: Choose best model\n",
    "\n",
    "Using the right split and the best model selected before, compute the generalization performance and assign it to a variable `AUC_best`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7780410d49dd0727bea11bcfa2ec75de",
     "grade": false,
     "grade_id": "cell-2d30a81f9c08fc5d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# assign to AUC_best the AUC of the best model selected before\n",
    "AUC_best = evaluator.evaluate(lr_pipeline1.transform(testing_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f01412786cef477f564cec439e3dbe9f",
     "grade": true,
     "grade_id": "cell-dc8fd7d7ce658642",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_approx_equal(AUC_best, \n",
    "                               0.9943181818181818, significant=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.5: Inference\n",
    "\n",
    "Use the pipeline 3 fitted above (`lr_pipeline3`) to create a Pandas dataframes that contain the most negative words and the most positive words. In particular, create a dataframe `positive_words` with the columns `word` and `weight` with the top 20 positive words, sorted by descending coefficient. Similarly create a `negative_words` Pandas dataframe with the top 20 negative words where the coefficient are sorted in ascending order. **Hint: follow the `sentiment_analysis.ipynb` notebook in the repo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b517fb83f670ea60b1fdb1580356a410",
     "grade": false,
     "grade_id": "cell-9fa9de453a36f8d2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create positive_words and negative_words pandas dataframe below\n",
    "vocabulary = lr_pipeline3.stages[0].stages[-2].vocabulary\n",
    "weights = lr_pipeline3.stages[-1].coefficients.toArray()\n",
    "coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})\n",
    "negative_words = coeffs_df.sort_values('weight').head(20)\n",
    "positive_words = coeffs_df.sort_values('weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should be as follows:\n",
    "\n",
    "`positive_words.head()`\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>word</th>\n",
    "      <th>weight</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>860</th>\n",
    "      <td>(edward</td>\n",
    "      <td>0.123131</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>129</th>\n",
    "      <td>baseball</td>\n",
    "      <td>0.107927</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>991</th>\n",
    "      <td>players?</td>\n",
    "      <td>0.092217</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>285</th>\n",
    "      <td>pitching</td>\n",
    "      <td>0.088141</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>969</th>\n",
    "      <td>fischer)</td>\n",
    "      <td>0.061178</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "`negative_words.head()`\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>word</th>\n",
    "      <th>weight</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>816</th>\n",
    "      <td>golchowy@alchemy.chem.utoronto.ca</td>\n",
    "      <td>-0.160207</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>882</th>\n",
    "      <td>nhl.</td>\n",
    "      <td>-0.148710</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>570</th>\n",
    "      <td>contact</td>\n",
    "      <td>-0.141691</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>236</th>\n",
    "      <td>playoff</td>\n",
    "      <td>-0.131673</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>810</th>\n",
    "      <td>olchowy)</td>\n",
    "      <td>-0.129368</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eb9d846088455b7ba9b3f30f5fc79cb7",
     "grade": true,
     "grade_id": "cell-5926b249c3d8d910",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_equal(set(positive_words.columns), {'weight', 'word'})\n",
    "np.testing.assert_equal(set(negative_words.columns), {'weight', 'word'})\n",
    "np.testing.assert_approx_equal(positive_words.weight.sum(), 1.1287686331251567, significant=1)\n",
    "np.testing.assert_approx_equal(negative_words.weight.sum(), -1.9525975400776723, significant=1)\n",
    "np.testing.assert_array_less(positive_words.weight.iloc[-1], positive_words.weight.iloc[0])\n",
    "np.testing.assert_array_less(negative_words.weight.iloc[0], negative_words.weight.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** Explain in simple terms what the top three positive words and the top three negative words might indicate about the prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a2675042a84222d120993ede4d08c5ab",
     "grade": true,
     "grade_id": "cell-ee3d1f6e740758dd",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "In our example, the dummy variable of baseball is set as 1 and that of hockey is set as 0. The coefficients in the regression will signify the relationship with baseball (since baseball is 1). \n",
    "\n",
    "For example, positive words in this case means that if these words are present in the email, the topic is baseball, whereas the negative words are those words in the email where the topic is not baseball (therefore it is hockey).\n",
    "\n",
    "From the top three positive words: (edward, baseball, players? we can understand that if these 3 words are present in the email, there are high chances that the email is about baseball. \n",
    "Edward could be the source of contact for the baseball games, and obviously the words 'baseball' and 'players' are mentioned in emails about baseball\n",
    "\n",
    "From the top three negative words: golchowy@alchemy.chem.utoronto.ca, nhl. and contact, we can understand that if these 3 words are present in the email, there are high chances that the email is not about baseball (and is about hockey)\n",
    "golchowy@alchemy.chem.utoronto.ca could be the email id from where the hockey emails are received, nhl could be a url from where newsletters/emails are recieved about hockey and the hockey emails might contain a high frequency of the word 'contact'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
